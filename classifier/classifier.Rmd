---
title: Classifier
output: pdf_document
---

```{r}
set.seed(42)

library(caret) # highly correlated features removal
library(tidyverse)
library(tidymodels)
library(e1071)
```

# Helpers

```{r}
train_svm <- function(
    training_set,
    testing_set,
    columns,
    kernel = "radial",
    gamma = if (is.vector(training_set)) 1 else 1 / ncol(training_set),
    cost = 1) {
  model <- svm(
    training_set[columns],
    training_set$class,
    kernel = kernel, type = "C-classification",
    gamma = gamma,
    cost = cost,
    probability = TRUE,
    cross = 10
  )

  if (is.null(testing_set)) {
    return(list(
      model = model
    ))
  }

  pred <- predict(model, testing_set[columns], probability = TRUE)
  set_with_preds <- testing_set %>%
    mutate(
      pred = pred,
      prob_good = attr(pred, "probabilities")[, "good"],
      prob_bad = attr(pred, "probabilities")[, "bad"]
    )

  cm <- confusionMatrix(
    set_with_preds$pred, set_with_preds$class,
    mode = "everything"
  )

  return(list(
    model = model,
    prediction_set = set_with_preds,
    cm = cm
  ))
}

train_glm <- function(training_set, testing_set, columns) {
  formula <- reformulate(colnames(training_set[columns]), "class")
  model <- glm(
    formula,
    training_set,
    family = "binomial"
  )
  pred <- predict(model, testing_set[columns], type = "response")
  set_with_preds <- testing_set %>%
    mutate(
      prob_good = pred,
      prob_bad = 1 - pred,
      pred = if_else(pred > .5, "good", "bad") %>%
        factor(levels = c("bad", "good"))
    )

  cm <- confusionMatrix(
    set_with_preds$pred, set_with_preds$class,
    mode = "everything"
  )

  return(list(
    model = model,
    prediction_set = set_with_preds,
    cm = cm
  ))
}

get_mismatch_details <- function(data_with_predictions) {
  plot <- data_with_predictions %>%
    ggplot(aes(x = prob_good, y = class, color = subcorpus)) +
    geom_jitter(height = 0.2, width = 0)
  print(plot)

  cat("Confusion matrices by subcorpora:\n")
  data_with_predictions %>%
    select(pred, class, subcorpus) %>%
    table() %>%
    print()

  cat("\n")

  deviations <- data_with_predictions %>%
    filter(pred != class) %>%
    mutate(abs_dev = abs(prob_good - 0.5)) %>%
    arrange(-abs_dev)

  cat("Greatest deviations:\n")
  deviations %>%
    select(abs_dev, prob_good, class, subcorpus, FileName) %>%
    mutate(across(c(prob_good, abs_dev), ~ round(.x, 3))) %>%
    print(n = round(nrow(data_with_predictions) / 5))

  cat("Names of highest-deviating documents:\n")
  highest_deviation_names <- deviations %>%
    filter(abs_dev >= 0.17) %>%
    arrange(-abs_dev) %>%
    pull(FileName)

  print(highest_deviation_names)

  return(list(
    deviations = deviations,
    highest_deviations = highest_deviation_names,
    plot = plot
  ))
}

analyze_outlier <- function(doc_name, variable_importances, dataset) {
  important_variables <- sort(variable_importances, decreasing = TRUE) %>%
    head(n = 16)
  varnames <- names(important_variables)

  varscores <- tibble(feat = character(), score = numeric())
  for (v in varnames) {
    vgood <- filter(dataset, class == "good")[[v]]
    vbad <- filter(dataset, class == "bad")[[v]]
    vdoc <- filter(dataset, FileName == doc_name)[[v]]
    docclass <- filter(dataset, FileName == doc_name)$class

    # so that good values are always greater
    if (mean(vgood) < mean(vbad)) {
      vbad <- -vbad
      vgood <- -vgood
      vdoc <- -vdoc
    }

    qgood <- quantile(vgood, probs = c(.25, .75))
    qbad <- quantile(vbad, probs = c(.25, .75))

    # -2 very bad, -1 bad, 0 medium, +1 good, +2 very good
    vscore <- sum(c(
      vdoc > qbad[[1]], vdoc > qbad[[2]], vdoc > qgood[[1]], vdoc > qgood[[2]]
    )) - 2

    varscores <- varscores %>% add_row(feat = v, score = vscore)
  }

  varscores <- varscores %>%
    mutate(verbose_score = case_when(
      score == -2 ~ "very bad",
      score == -1 ~ "bad",
      score == 1 ~ "good",
      score == 2 ~ "very good",
      .default = "medium"
    )) %>%
    rowid_to_column("rank") %>%
    select(rank, everything())

  cat(paste("class", docclass, "and:\n"))
  if (docclass == "good") {
    print(
      varscores %>%
        filter(score < 0) %>%
        select(rank, feat, verbose_score) %>%
        as.data.frame()
    )
  } else {
    print(
      varscores %>%
        filter(score > 0) %>%
        select(rank, feat, verbose_score) %>%
        as.data.frame()
    )
  }
  cat("even though:\n")
  if (docclass == "good") {
    print(
      varscores %>%
        filter(score >= 0) %>%
        select(rank, feat, verbose_score) %>%
        as.data.frame()
    )
  } else {
    print(
      varscores %>%
        filter(score <= 0) %>%
        select(rank, feat, verbose_score) %>%
        as.data.frame()
    )
  }

  dmut <- dataset %>%
    select(KUK_ID, FileName, class, all_of(varnames)) %>%
    mutate(across(all_of(varnames), ~ scale(.x))) %>%
    pivot_longer(
      all_of(varnames),
      names_to = "feature", values_to = "value"
    ) %>%
    mutate(across(value, ~ .x[, 1])) %>%
    mutate(across(feature, ~ factor(.x, levels = varnames)))


  cat(
    nrow(dmut %>% filter(value > 5)),
    "observation(s) removed from the plot\n"
  )
  dmutf <- dmut %>% filter(value <= 5)

  plot <- dmutf %>%
    ggplot(aes(x = class, y = value)) +
    facet_wrap(~feature) +
    geom_boxplot() +
    geom_point(
      data = dmut %>% filter(FileName == doc_name), color = "red", size = 5
    ) +
    labs(y = "measurements (scaled)")

  return(plot)
}
```

# Load and tidy data

```{r}
pretty_names <- read_csv("../feat_name_mapping.csv")

prettify_feat_name <- function(x) {
  name <- pull(pretty_names %>%
    filter(name_orig == x), name_pretty)
  if (length(name) == 1) {
    return(name)
  } else {
    return(x)
  }
}

prettify_feat_name_vector <- function(x) {
  map(
    x,
    prettify_feat_name
  ) %>% unlist()
}


data <- read_csv("../measurements/measurements.csv")

.firstnonmetacolumn <- 18

data_no_nas <- data %>%
  select(!c(
    fpath,
    # KUK_ID,
    # FileName,
    FolderPath,
    # subcorpus,
    DocumentTitle,
    ClarityPursuit,
    # Readability,
    SyllogismBased,
    SourceDB
  )) %>%
  # replace -1s in variation coefficients with NAs
  mutate(across(c(
    `RuleDoubleAdpos.max_allowable_distance.v`,
    `RuleTooManyNegations.max_negation_frac.v`,
    `RuleTooManyNegations.max_allowable_negations.v`,
    `RuleTooManyNominalConstructions.max_noun_frac.v`,
    `RuleTooManyNominalConstructions.max_allowable_nouns.v`,
    `RuleCaseRepetition.max_repetition_count.v`,
    `RuleCaseRepetition.max_repetition_frac.v`,
    `RulePredSubjDistance.max_distance.v`,
    `RulePredObjDistance.max_distance.v`,
    `RuleInfVerbDistance.max_distance.v`,
    `RuleMultiPartVerbs.max_distance.v`,
    `RuleLongSentences.max_length.v`,
    `RulePredAtClauseBeginning.max_order.v`,
    `mattr.v`,
    `maentropy.v`
  ), ~ na_if(.x, -1))) %>%
  # replace NAs with 0s
  replace_na(list(
    RuleGPcoordovs = 0,
    RuleGPdeverbaddr = 0,
    RuleGPpatinstr = 0,
    RuleGPdeverbsubj = 0,
    RuleGPadjective = 0,
    RuleGPpatbenperson = 0,
    RuleGPwordorder = 0,
    RuleDoubleAdpos = 0,
    RuleDoubleAdpos.max_allowable_distance.v = 0,
    RuleAmbiguousRegards = 0,
    RuleReflexivePassWithAnimSubj = 0,
    RuleTooManyNegations = 0,
    RuleTooManyNegations.max_negation_frac.v = 0,
    RuleTooManyNegations.max_allowable_negations.v = 0,
    RuleTooManyNominalConstructions.max_noun_frac.v = 0,
    RuleTooManyNominalConstructions.max_allowable_nouns.v = 0,
    RuleFunctionWordRepetition = 0,
    RuleCaseRepetition.max_repetition_count.v = 0,
    RuleCaseRepetition.max_repetition_frac.v = 0,
    RuleWeakMeaningWords = 0,
    RuleAbstractNouns = 0,
    RuleRelativisticExpressions = 0,
    RuleConfirmationExpressions = 0,
    RuleRedundantExpressions = 0,
    RuleTooLongExpressions = 0,
    RuleAnaphoricReferences = 0,
    RuleLiteraryStyle = 0,
    RulePassive = 0,
    RulePredSubjDistance = 0,
    RulePredSubjDistance.max_distance.v = 0,
    RulePredObjDistance = 0,
    RulePredObjDistance.max_distance.v = 0,
    RuleInfVerbDistance = 0,
    RuleInfVerbDistance.max_distance.v = 0,
    RuleMultiPartVerbs = 0,
    RuleMultiPartVerbs.max_distance.v = 0,
    RuleLongSentences.max_length.v = 0,
    RulePredAtClauseBeginning.max_order.v = 0,
    RuleVerbalNouns = 0,
    RuleDoubleComparison = 0,
    RuleWrongValencyCase = 0,
    RuleWrongVerbonominalCase = 0,
    RuleIncompleteConjunction = 0
  )) %>%
  # replace NAs with medians
  mutate(across(c(
    RuleDoubleAdpos.max_allowable_distance,
    RuleTooManyNegations.max_negation_frac,
    RuleTooManyNegations.max_allowable_negations,
    RulePredSubjDistance.max_distance,
    RulePredObjDistance.max_distance,
    RuleInfVerbDistance.max_distance,
    RuleMultiPartVerbs.max_distance
  ), ~ coalesce(., median(., na.rm = TRUE)))) %>%
  # merge GPs
  mutate(
    GPs = RuleGPcoordovs +
      RuleGPdeverbaddr +
      RuleGPpatinstr +
      RuleGPdeverbsubj +
      RuleGPadjective +
      RuleGPpatbenperson +
      RuleGPwordorder
  ) %>%
  select(!c(
    RuleGPcoordovs,
    RuleGPdeverbaddr,
    RuleGPpatinstr,
    RuleGPdeverbsubj,
    RuleGPadjective,
    RuleGPpatbenperson,
    RuleGPwordorder
  ))

data_clean <- data_no_nas %>%
  # norm data expected to correlate with text length
  mutate(across(c(
    GPs,
    RuleDoubleAdpos,
    RuleAmbiguousRegards,
    RuleFunctionWordRepetition,
    RuleWeakMeaningWords,
    RuleAbstractNouns,
    RuleRelativisticExpressions,
    RuleConfirmationExpressions,
    RuleRedundantExpressions,
    RuleTooLongExpressions,
    RuleAnaphoricReferences,
    RuleLiteraryStyle,
    RulePassive,
    RuleVerbalNouns,
    RuleDoubleComparison,
    RuleWrongValencyCase,
    RuleWrongVerbonominalCase,
    RuleIncompleteConjunction,
    num_hapax,
    RuleReflexivePassWithAnimSubj,
    RuleTooManyNominalConstructions,
    RulePredSubjDistance,
    RuleMultiPartVerbs,
    RulePredAtClauseBeginning
  ), ~ .x / word_count)) %>%
  mutate(across(c(
    RuleTooFewVerbs,
    RuleTooManyNegations,
    RuleCaseRepetition,
    RuleLongSentences,
    RulePredObjDistance,
    RuleInfVerbDistance
  ), ~ .x / sent_count)) %>%
  # remove variables identified as text-length dependent
  select(!c(
    RuleTooFewVerbs,
    RuleTooManyNegations,
    RuleTooManyNominalConstructions,
    RuleCaseRepetition,
    RuleLongSentences,
    RulePredAtClauseBeginning,
    syllab_count,
    char_count
  )) %>%
  # remove variables identified as unreliable
  select(!c(
    RuleAmbiguousRegards,
    RuleFunctionWordRepetition,
    RuleDoubleComparison,
    RuleWrongValencyCase,
    RuleWrongVerbonominalCase
  )) %>%
  # remove further variables belonging to the 'acceptability' category
  select(!c(RuleIncompleteConjunction)) %>%
  # remove artificially limited variables
  select(!c(
    RuleCaseRepetition.max_repetition_frac,
    RuleCaseRepetition.max_repetition_frac.v
  )) %>%
  # remove variables with too many NAs
  select(!c(
    RuleDoubleAdpos.max_allowable_distance,
    RuleDoubleAdpos.max_allowable_distance.v
  )) %>%
  mutate(across(c(
    class,
    FileFormat,
    subcorpus,
    DocumentVersion,
    LegalActType,
    Objectivity,
    AuthorType,
    RecipientType,
    RecipientIndividuation,
    Anonymized
  ), ~ as.factor(.x)))

# no NAs should be present now
data_clean[!complete.cases(data_clean[.firstnonmetacolumn:ncol(data_clean)]), ]

colnames(data_clean) <- prettify_feat_name_vector(colnames(data_clean))

data_scaled <- data_clean %>%
  mutate(across(all_of(.firstnonmetacolumn:ncol(data_clean)), ~ scale(.x)[, 1]))

data_stratified <- data_scaled %>%
  unite("strata", c("class", "subcorpus"), remove = FALSE)
```

# Important features identification

```{r}
feature_importances <- read_csv("../importance_measures/featcomp.csv")

selected_features_names <- feature_importances %>%
  filter(kw_sel) %>%
  pull(Variable)

variable_importances <- feature_importances %>%
  filter(kw_sel) %>%
  pull(kw_epsilon2)
names(variable_importances) <- selected_features_names
```

# Formulas

```{r}
columns_all <- colnames(data_stratified)[
  (.firstnonmetacolumn + 1):ncol(data_stratified)
]
columns_cleaned <- columns_all[!(columns_all %in% c("atl", "cli"))]
# columns_readabilty_forms <- c("ari", "cli", "fkgl", "fre", "gf", "smog")
columns_readabilty_forms <- c("ari", "fkgl", "fre", "gf", "smog")

correlating90 <- findCorrelation(
  cor(data_stratified[columns_all]),
  cutoff = 0.9, verbose = TRUE, names = TRUE
)

columns_notcorrelating90 <- c()
for (col in columns_all) {
  if (!(col %in% correlating90)) {
    columns_notcorrelating90 <- c(columns_notcorrelating90, col)
  }
}

correlating85 <- findCorrelation(
  cor(data_stratified[columns_all]),
  cutoff = 0.85, verbose = TRUE, names = TRUE
)

columns_notcorrelating85 <- c()
for (col in columns_all) {
  if (!(col %in% correlating85)) {
    columns_notcorrelating85 <- c(columns_notcorrelating85, col)
  }
}

correlating75 <- findCorrelation(
  cor(data_stratified[columns_all]),
  cutoff = 0.75, verbose = TRUE, names = TRUE
)

columns_notcorrelating75 <- c()
for (col in columns_all) {
  if (!(col %in% correlating75)) {
    columns_notcorrelating75 <- c(columns_notcorrelating75, col)
  }
}
```

# Hyperparameters

```{r}
# colsids <- c(
#   "all", "notcorrelating90",
#   "notcorrelating85", "notcorrelating75"
# )
# colsets <- list(
#   columns_all, columns_notcorrelating90,
#   columns_notcorrelating85, columns_notcorrelating75
# )
colsids <- c("all", "cleaned", "readforms")
colsets <- list(columns_all, columns_cleaned, columns_readabilty_forms)
```

# Splits and folds

```{r}
.splitprop <- 3 / 4

split <- initial_split(data_stratified, .splitprop, strata = strata)

training_set <- training(split)
testing_set <- testing(split)

training_set %>%
  select(class) %>%
  table()
testing_set %>%
  select(class) %>%
  table()

training_set %>%
  select(subcorpus, class) %>%
  table()
testing_set %>%
  select(subcorpus, class) %>%
  table()
```

# Tune

```{r tune}
tune_res <- tibble(
  columns = character(),
  kernel = character(),
  gamma = numeric(),
  cost = numeric(),
  error = numeric(),
  dispersion = numeric()
)

# for (coli in seq_along(colsets)) {
#   colsid <- colsids[coli]
#   columns <- colsets[[coli]]

#   message("tune linear on ", colsid)
#   tune_linear <- tune.svm(training_set[columns], training_set$class,
#     cost = 10^(-3:1),
#     kernel = "linear"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_linear$performances %>%
#       mutate(kernel = "linear", columns = colsid, gamma = 0))

#   message("tune radial on ", colsid)
#   tune_radial <- tune.svm(training_set[columns], training_set$class,
#     gamma = 10^(-3:3),
#     cost = c(0.01, 0.1, 1, 10, 100, 1000),
#     kernel = "radial"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_radial$performances %>%
#       mutate(kernel = "radial", columns = colsid))

#   message("tune polynomial3 on ", colsid)
#   tune_polynomial <- tune.svm(training_set[columns], training_set$class,
#     gamma = 10^(-3:0),
#     degree = 3,
#     cost = 10^(-3:1),
#     kernel = "polynomial"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_polynomial$performances %>%
#       mutate(kernel = "polynomial3", columns = colsid))


#   message("tune polynomial4 on ", colsid)
#   tune_polynomial <- tune.svm(training_set[columns], training_set$class,
#     gamma = 10^(-3:-1),
#     degree = 4,
#     cost = 10^(-3:1),
#     kernel = "polynomial"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_polynomial$performances %>%
#       mutate(kernel = "polynomial4", columns = colsid))


#   message("tune polynomial5 on ", colsid)
#   tune_polynomial <- tune.svm(training_set[columns], training_set$class,
#     gamma = 10^(-3:-1),
#     degree = 5,
#     cost = 10^(-3:0),
#     kernel = "polynomial"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_polynomial$performances %>%
#       mutate(kernel = "polynomial5", columns = colsid))


#   message("tune sigmoid on ", colsid)
#   tune_sigmoid <- tune.svm(training_set[columns], training_set$class,
#     gamma = 10^(-3:3),
#     cost = 10^(-3:3),
#     kernel = "sigmoid"
#   )
#   tune_res <- tune_res %>%
#     bind_rows(tune_sigmoid$performances %>%
#       mutate(kernel = "sigmoid", columns = colsid))
# }

# tune_res %>% write_csv("tune_results.csv")
tune_res <- read_csv("tune_results.csv")

tune_res %>%
  arrange(error, -dispersion)

tune_res %>%
  arrange(error + dispersion)

tune_res %>%
  filter(columns == "all") %>%
  arrange(error, -dispersion)

tune_res %>%
  filter(str_detect(columns, "notcorrelating.*")) %>%
  arrange(error, -dispersion)
```

```{r tune-report, fig.width=12, fig.height=12, fig.align='center'}
tune_res %>%
  mutate(across(gamma, as.factor)) %>%
  ggplot(aes(
    x = cost, y = error, ymin = error - dispersion,
    ymax = error + dispersion, color = gamma, fill = gamma
  )) +
  geom_point() +
  geom_line() +
  geom_ribbon(alpha = 0.1) +
  scale_x_log10() +
  facet_grid(kernel ~ columns) +
  theme(legend.position = "bottom")
```

best:

- columns: all
- kernel: radial
- gamma: 0.01
- cost: 1

# SVM

```{r}
set.seed(42)

model_all <- train_svm(
  training_set, testing_set, columns_all, "radial",
  gamma = 0.01, cost = 1
)
model_all$cm

mismatches_all <- get_mismatch_details(model_all$prediction_set)
mismatches_all$plot +
  theme_bw() +
  labs(y = "true class", x = "probability estimate")
ggsave("model_all_probabilities.pdf")
```

```{r outliers-all, fig.height=14, fig.width=10, fig.align='center'}
for (doc in mismatches_all$highest_deviations) {
  doc_row <- mismatches_all$deviations %>% filter(FileName == doc)
  cat(paste(doc, "/", doc_row["subcorpus"][[1]], "\n"))
  cat("KUK_ID:", doc_row["KUK_ID"][[1]], "\n")
  cat("dev:", doc_row["abs_dev"][[1]] %>% round(3), "\n")
  cat("Readability:", doc_row["Readability"][[1]], "\n")

  plt <- analyze_outlier(doc, variable_importances, data_clean) + theme_bw()
  print(plt)
  ggsave(
    paste(c("outlier_all_", doc_row["KUK_ID"][[1]], ".pdf"), collapse = ""),
    plt,
    width = 8,
    height = 8
  )
}
```

# SVM cleaned


```{r}
set.seed(42)

model_cleaned <- train_svm(
  training_set, testing_set, columns_cleaned, "radial",
  gamma = 0.01, cost = 1
)
model_cleaned$cm

mismatches_cleaned <- get_mismatch_details(model_cleaned$prediction_set)
mismatches_cleaned$plot +
  theme_bw() +
  labs(y = "true class", x = "probability estimate")
ggsave("model_cleaned_probabilities.pdf")
```

```{r outliers-cleaned, fig.height=14, fig.width=10, fig.align='center'}
variable_importances_cleaned <- variable_importances[
  names(variable_importances) %in% columns_cleaned
]

for (doc in mismatches_cleaned$highest_deviations) {
  doc_row <- mismatches_cleaned$deviations %>% filter(FileName == doc)
  cat(paste(doc, "/", doc_row["subcorpus"][[1]], "\n"))
  cat("KUK_ID:", doc_row["KUK_ID"][[1]], "\n")
  cat("dev:", doc_row["abs_dev"][[1]] %>% round(3), "\n")
  cat("Readability:", doc_row["Readability"][[1]], "\n")

  plt <- analyze_outlier(doc, variable_importances_cleaned, data_clean) +
    theme_bw()
  print(plt)
  ggsave(
    paste(
      c("outlier_cleaned_", doc_row["KUK_ID"][[1]], ".pdf"),
      collapse = ""
    ), plt,
    width = 8,
    height = 8
  )
}
```


# SVM readability formulas

```{r}
set.seed(42)

model_rf <- train_svm(
  training_set, testing_set, columns_readabilty_forms, "radial",
  gamma = 0.01, cost = 1
)
model_rf$cm
```